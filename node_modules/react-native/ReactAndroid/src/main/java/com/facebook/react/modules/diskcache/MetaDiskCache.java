//*** licence placeholder ***//

package com.facebook.react.modules.diskcache;

import android.os.Process;
import android.util.Log;
import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import bolts.Task;
import com.facebook.binaryresource.BinaryResource;
import com.facebook.binaryresource.FileBinaryResource;
import com.facebook.cache.common.CacheErrorLogger;
import com.facebook.cache.common.CacheEvent;
import com.facebook.cache.common.CacheEventListener;
import com.facebook.cache.common.CacheKey;
import com.facebook.cache.common.WriterCallback;
import com.facebook.cache.disk.DiskCacheConfig;
import com.facebook.cache.disk.FileCache;
import com.facebook.common.disk.DiskTrimmable;
import com.facebook.common.disk.DiskTrimmableRegistry;
import com.facebook.common.internal.Closeables;
import com.facebook.common.logging.FLog;
import com.facebook.common.memory.MemoryTrimType;
import com.facebook.common.memory.MemoryTrimmable;
import com.facebook.common.memory.MemoryTrimmableRegistry;
import com.facebook.common.memory.PooledByteBuffer;
import com.facebook.common.memory.PooledByteBufferFactory;
import com.facebook.common.memory.PooledByteStreams;
import com.facebook.common.references.CloseableReference;
import com.facebook.common.references.ResourceReleaser;
import com.facebook.imagepipeline.core.DiskStorageCacheFactory;
import com.facebook.imagepipeline.core.DynamicDefaultDiskStorageFactory;
import com.facebook.imagepipeline.core.MemoryChunkType;
import com.facebook.imagepipeline.core.PriorityThreadFactory;
import com.facebook.imagepipeline.memory.MemoryChunk;
import com.facebook.imagepipeline.memory.MemoryPooledByteBuffer;
import com.facebook.imagepipeline.memory.PoolConfig;
import com.facebook.imagepipeline.memory.PoolFactory;
import com.facebook.infer.annotation.Assertions;
import com.facebook.react.common.MapBuilder;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.lang.ref.WeakReference;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.concurrent.Callable;
import java.util.concurrent.CancellationException;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.CopyOnWriteArrayList;
import java.util.concurrent.Executor;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicBoolean;

public class MetaDiskCache {

  public enum EventType {
    EXCEPTION,
    DISK_CACHE_DESTROY,
    EVALUATE_JS_WITH_CACHE,
    GET_CACHE_NULL,
    GET_INVALID_CACHE_VERSION,
    GET_INVALID_CRC,
    GET_CACHE_HIT,
    GET_CACHE_MISS,
    PUT_RESULT_EXCEPTION,
    PUT_CACHE_UPDATED,
    PUT_META_VERIFY_ERROR,
    RUN_FINISHED,
    ON_HIT,
    ON_MISS,
    ON_WRITE_ATTEMPT,
    ON_WRITE_SUCCESS,
    ON_READ_EXCEPTION,
    ON_WRITE_EXCEPTION,
    ON_EVICTION,
    ON_CLEARED
  }

  public static final Class<?> TAG = MetaDiskCache.class;
  private static final int NUM_IO_BOUND_THREADS = 2; // TODO:通过参数配置，是否可以减少为1，减少OOM的概率
  private static final String STORAGE_PATH = File.separator + "MetaDiskCache";
  private static final int MAX_CACHE_SIZE = 1024 * 1024 * 100; // 100M
  private static final int DB_VERSION = 1;

  public static final String EVENT_PARAM_KEY = "Key";
  public static final String EVENT_PARAM_FROM = "From";
  public static final String EVENT_PARAM_RESULT = "Result";
  public static final String EVENT_PARAM_MESSAGE = "Message";
  public static final String EVENT_PARAM_EXCEPTION = "Exception";
  public static final String EVENT_PARAM_CATEGORY = "Category";

  private final FileCache mFileCache;
  private final PooledByteBufferFactory mPooledByteBufferFactory;
  private final PooledByteStreams mPooledByteStreams;
  private final Executor mIOExecutor;
  private final StagingMap mStagingMap;

  private final PoolFactory mPoolFactory;
  private final Map<Integer, WeakReference<MemoryTrimmable>> mMemoryPool;

  // use CacheEventListener and CacheErrorLogger instead
  private final MetaDiskCacheEventHandler mMetaDiskCacheEventHandler;
  private final CacheErrorLogger mCacheErrorLogger;
  private final CacheEventListener mCacheEventListener;
  private static final ResourceReleaser<MappedMemoryChunk> MMAP_BUFFER_RELEASER =
      value -> {
        try {
          Closeables.close(value, true);
        } catch (IOException ioe) {
          // This will not happen, Closeable.close swallows and logs IOExceptions
        }
      };

  public static MetaDiskCache createMetaDiskCache(DiskCacheConfig diskCacheConfig,
      @Nullable final String path, @Nullable MetaDiskCacheEventHandler metaDiskCacheEventHandler) {
    return new MetaDiskCache(diskCacheConfig, path, metaDiskCacheEventHandler);
  }

  private MetaDiskCache(@Nullable DiskCacheConfig diskCacheConfig, @Nullable final String path,
      @Nullable MetaDiskCacheEventHandler metaDiskCacheEventHandler) {
    Assertions.assertCondition(diskCacheConfig != null || path != null);

    mPoolFactory = new PoolFactory(buildPoolConfig());
    mMemoryPool = new ConcurrentHashMap<>();
    mIOExecutor = Executors.newFixedThreadPool(
        NUM_IO_BOUND_THREADS,
        new PriorityThreadFactory(
            Process.THREAD_PRIORITY_BACKGROUND, "DiskCacheIOExecutor", true));
    if (diskCacheConfig == null) {
      diskCacheConfig = buildDiskCacheConfig(path);
    }
    mFileCache = DiskStorageCacheFactory.buildDiskStorageCache(diskCacheConfig,
        new DynamicDefaultDiskStorageFactory().get(diskCacheConfig), mIOExecutor);

    // Use direct byte buffer for IO at Java/C++
    mPooledByteBufferFactory = mPoolFactory
        .getPooledByteBufferFactory(MemoryChunkType.BUFFER_MEMORY);
    mPooledByteStreams = mPoolFactory.getPooledByteStreams();
    mMetaDiskCacheEventHandler = metaDiskCacheEventHandler;
    mStagingMap = StagingMap.getInstance();

    mCacheErrorLogger = new CacheErrorLogger() {
      @Override
      public void logError(CacheErrorCategory category, Class<?> clazz, String message,
          @javax.annotation.Nullable Throwable throwable) {
        FLog.e(TAG, "Error:" + category.name() + ":" + message);
        if (mMetaDiskCacheEventHandler != null) {
          mMetaDiskCacheEventHandler
              .handleDiskCacheMessage(Log.ERROR, EventType.EXCEPTION.name(), MapBuilder
                  .of(EVENT_PARAM_CATEGORY, category.name(), EVENT_PARAM_MESSAGE, message,
                      EVENT_PARAM_EXCEPTION,
                      throwable != null ? throwable.toString() : "unknown Exception"), throwable);
        }
      }
    };
    mCacheEventListener = new CacheEventListener() {
      @Override
      public void onHit(CacheEvent cacheEvent) {
        logCacheEvent(Log.INFO, EventType.ON_HIT.name(), cacheEvent);
      }

      @Override
      public void onMiss(CacheEvent cacheEvent) {
        logCacheEvent(Log.INFO, EventType.ON_MISS.name(), cacheEvent);
      }

      @Override
      public void onWriteAttempt(CacheEvent cacheEvent) {
        logCacheEvent(Log.INFO, EventType.ON_WRITE_ATTEMPT.name(), cacheEvent);

      }

      @Override
      public void onWriteSuccess(CacheEvent cacheEvent) {
        logCacheEvent(Log.INFO, EventType.ON_WRITE_SUCCESS.name(), cacheEvent);

      }

      @Override
      public void onReadException(CacheEvent cacheEvent) {
        logCacheEvent(Log.ERROR, EventType.ON_READ_EXCEPTION.name(), cacheEvent);

      }

      @Override
      public void onWriteException(CacheEvent cacheEvent) {
        logCacheEvent(Log.ERROR, EventType.ON_WRITE_EXCEPTION.name(), cacheEvent);

      }

      @Override
      public void onEviction(CacheEvent cacheEvent) {
        logCacheEvent(Log.INFO, EventType.ON_EVICTION.name(), cacheEvent);

      }

      @Override
      public void onCleared() {
        logCacheEvent(Log.INFO, EventType.ON_CLEARED.name(), null);
      }

      private void logCacheEvent(int level, final String type, final CacheEvent cacheEvent) {
        final Map<String, String> message = cacheEventToMap(cacheEvent);
        final Throwable throwable = (cacheEvent != null ? cacheEvent.getException() : null);
        if (level == Log.ERROR) {
          FLog.e(TAG, message.toString(), throwable);
        } else {
          FLog.i(TAG, message.toString());
        }
        if (mMetaDiskCacheEventHandler != null) {
          mMetaDiskCacheEventHandler.handleDiskCacheMessage(level, type, message, throwable);
        }
      }

      private Map<String, String> cacheEventToMap(final CacheEvent cacheEvent) {
        if (cacheEvent != null) {
          return MapBuilder.of(EVENT_PARAM_KEY,
              cacheEvent.getCacheKey() != null ? cacheEvent.getCacheKey().getUriString()
                  : cacheEvent.getResourceId(), EVENT_PARAM_EXCEPTION,
              cacheEvent.getException() != null ? cacheEvent.getException()
                  .toString() : "unknownException");
        } else {
          return new HashMap<>();
        }
      }
    };

  }

  public void runOnDiskCacheThread(Runnable runnable) {
    try {
      mIOExecutor.execute(runnable);
    } catch (Exception exception) {
      FLog.e(TAG, "runOnDiskCacheThread: " + exception.toString());
    }
  }

  /**
   * Update timestamp of key
   *
   * @param key
   */
  public void probe(CacheKey key) {
    Task.call((Callable<Void>) () -> {
      mFileCache.probe(key);
      return null;
    });
  }

  /**
   * Returns true if the key is in the in-memory key index.
   * <p>
   * Not guaranteed to be correct. The cache may yet have this key even if this returns false. But
   * if it returns true, it definitely has it.
   * <p>
   * Avoids a disk read.
   */
  public boolean containsSync(CacheKey key) {
    return mStagingMap.containsKey(key) || mFileCache.hasKeySync(key);
  }

  public boolean checkDisk(CacheKey key) {
    return mFileCache.hasKeySync(key);
  }

  /**
   * Performs a key-value look up in the disk cache. If no value is found in the staging area, then
   * disk cache checks are scheduled on a background thread. Any error manifests itself as a cache
   * miss, i.e. the returned Task resolves to false.
   *
   * @param key
   * @return Task that resolves to true if an element is found, or false otherwise
   */
  public Task<Boolean> contains(final CacheKey key) {
    if (containsSync(key)) {
      return Task.forResult(true);
    }
    return containsAsync(key);
  }

  private Task<Boolean> containsAsync(final CacheKey key) {
    try {
      return Task.call(
          () -> checkInStagingAreaAndFileCache(key),
          mIOExecutor);
    } catch (Exception exception) {
      // Log failure
      // TODO: 3697790
      FLog.w(
          TAG,
          exception,
          "Failed to schedule disk-cache read for %s",
          key.getUriString());
      return Task.forError(exception);
    }
  }

  /**
   * Performs disk cache check synchronously.
   *
   * @param key
   * @return true if the key is found in disk cache else false
   */
  public boolean diskCheckSync(final CacheKey key) {
    if (containsSync(key)) {
      return true;
    }
    return checkInStagingAreaAndFileCache(key);
  }

  /**
   * Load entry into staging ahead and get later instantly. The preload item will be overwritten by
   * put/remove operations. WARNING: Users have to call clearPreload to release preloaded entries.
   *
   * @param key
   * @param isCancelled
   */
  public void preload(CacheKey key, AtomicBoolean isCancelled) {
    if (mStagingMap.containsKey(key)) {
      return;
    }

    get(key, isCancelled).continueWith(task -> {
      FileEntry fileEntry = task.getResult();
      fileEntry.setPreload(true);
      mStagingMap.putIfNotExist(key, fileEntry);

      return null;
    });
  }

  public void clearPreload(CacheKey key) {
    mStagingMap.clearPreload(key);
  }

  /**
   * Performs key-value look up in disk cache. If value is not found in disk cache staging area then
   * disk cache read is scheduled on background thread. Any error manifests itself as cache miss,
   * i.e. the returned task resolves to null.
   *
   * @param key
   * @return Task that resolves to cached element or null if one cannot be retrieved; returned task
   * never rethrows any exception
   */
  public Task<FileEntry> get(CacheKey key, AtomicBoolean isCancelled) {
    final FileEntry fileEntry = mStagingMap.get(key);
    if (fileEntry != null) {
      fileEntry.setFromMemory(true);
      return foundFileEntry(key, fileEntry);
    }
    return getAsync(key, isCancelled);
  }

  /**
   * Synchronized version of get
   *
   * @param key
   * @return
   */
  public FileEntry getSync(CacheKey key) {
    FileEntry result = mStagingMap.get(key);
    if (result != null) {
      result.setFromMemory(true);
      FLog.v(TAG, "Found entry for %s in staging area", key.getUriString());
    } else {
      FLog.v(TAG, "Did not find entry for %s in staging area", key.getUriString());

      try {
        final PooledByteBuffer buffer = readFromDiskCache(key);
        if (buffer == null) {
          return null;
        }
        CloseableReference<PooledByteBuffer> ref = CloseableReference.of(buffer);
        try {
          result = FileEntry.create(ref);
        } finally {
          CloseableReference.closeSafely(ref);
        }
      } catch (Exception exception) {
        mCacheErrorLogger
            .logError(CacheErrorLogger.CacheErrorCategory.GENERIC_IO, TAG, "getSync", exception);
        return null;
      }
    }
    return result;
  }

  /**
   * Performs key-value loop up in staging area and file cache. Any error manifests itself as a
   * miss, i.e. returns false.
   *
   * @param key
   * @return true if the entry is found in staging area or File cache, false if not found
   */
  private boolean checkInStagingAreaAndFileCache(final CacheKey key) {
    FileEntry result = mStagingMap.get(key);
    if (result != null) {
      result.close();
      FLog.v(TAG, "Found entry for %s in staging area", key.getUriString());
      return true;
    } else {
      FLog.v(TAG, "Did not find entry for %s in staging area", key.getUriString());
      try {
        return mFileCache.hasKey(key);
      } catch (Exception exception) {
        mCacheErrorLogger.logError(CacheErrorLogger.CacheErrorCategory.GENERIC_IO, TAG,
            "checkInStagingAreaAndFileCache", exception);
        return false;
      }
    }
  }

  private Task<FileEntry> getAsync(final CacheKey key, final AtomicBoolean isCancelled) {
    try {
      return Task.call(
          () -> {
            if (isCancelled.get()) {
              throw new CancellationException();
            }
            FileEntry result = getSync(key);
            if (Thread.interrupted()) {
              FLog.v(TAG, "Host thread was interrupted, decreasing reference count");
              if (result != null) {
                result.close();
              }
              throw new InterruptedException();
            } else {
              return result;
            }
          },
          mIOExecutor);
    } catch (Exception exception) {
      // Log failure
      // TODO: 3697790
      FLog.w(
          TAG,
          exception,
          "Failed to schedule disk-cache read for %s",
          key.getUriString());
      mCacheErrorLogger
          .logError(CacheErrorLogger.CacheErrorCategory.GENERIC_IO, TAG, "getAsync", exception);
      return Task.forError(exception);
    }
  }

  /**
   * Associates fileEntry with given key in disk cache. Disk write is performed on background
   * thread, so the caller of this method is not blocked
   */
  public void put(
      final CacheKey key,
      FileEntry fileEntry) {
    try {
      Assertions.assertNotNull(key);
      Assertions.assertCondition(FileEntry.isValid(fileEntry));

      // Store fileEntry in staging area
      mStagingMap.put(key, fileEntry);

      try {

        mIOExecutor.execute(
            () -> {
              try {
                writeToDiskCache(key, fileEntry);
              } finally {
                mStagingMap.remove(key, fileEntry);
                FileEntry.closeSafely(fileEntry);
              }
            });
      } catch (Exception exception) {
        // We failed to enqueue cache write. Log failure and decrement ref count
        // TODO: 3697790
        FLog.w(TAG, exception, "Failed to schedule disk-cache write for %s", key.getUriString());
        mCacheErrorLogger
            .logError(CacheErrorLogger.CacheErrorCategory.GENERIC_IO, TAG, "put", exception);
        mStagingMap.remove(key, fileEntry);
        FileEntry.closeSafely(fileEntry);
      }
    } finally {
    }
  }

  public Task<Void> putAsync(
      final CacheKey key,
      FileEntry fileEntry) {
    try {
      Assertions.assertNotNull(key);
      Assertions.assertCondition(FileEntry.isValid(fileEntry));

      // Store fileEntry in staging area
      mStagingMap.put(key, fileEntry);

      return Task.call(new Callable<Void>() {
        @Override
        public Void call() throws Exception {

          try {
            try {
              writeToDiskCache(key, fileEntry);
            } finally {
              mStagingMap.remove(key, fileEntry);
              FileEntry.closeSafely(fileEntry);
            }
          } catch (Exception exception) {
            // We failed to enqueue cache write. Log failure and decrement ref count
            // TODO: 3697790
            FLog.w(TAG, exception, "Failed to schedule disk-cache write for %s",
                key.getUriString());
            mStagingMap.remove(key, fileEntry);
            FileEntry.closeSafely(fileEntry);
            mCacheErrorLogger
                .logError(CacheErrorLogger.CacheErrorCategory.GENERIC_IO, TAG, "putAsync",
                    exception);
          }
          return null;
        }
      }, mIOExecutor);
    } catch (Exception exception) {
      mCacheErrorLogger
          .logError(CacheErrorLogger.CacheErrorCategory.GENERIC_IO, TAG, "putAsync", exception);
      return Task.forError(exception);
    } finally {
    }
  }

  /**
   * Removes the item from the disk cache and the staging area.
   */
  public Task<Void> remove(final CacheKey key) {
    Assertions.assertNotNull(key);
    mStagingMap.remove(key);
    try {
      return Task.call(
          () -> {
            mStagingMap.remove(key);
            mFileCache.remove(key);
            return null;
          },
          mIOExecutor);
    } catch (Exception exception) {
      // Log failure
      // TODO: 3697790
      FLog.w(TAG, exception, "Failed to schedule disk-cache remove for %s", key.getUriString());
      mCacheErrorLogger
          .logError(CacheErrorLogger.CacheErrorCategory.GENERIC_IO, TAG, "remove", exception);
      return Task.forError(exception);
    }
  }

  /**
   * Clears the disk cache and the staging area.
   */
  public Task<Void> clearAll() {
    mStagingMap.clearAll();
    try {
      return Task.call(
          () -> {
            mStagingMap.clearAll();
            mFileCache.clearAll();
            return null;
          },
          mIOExecutor);
    } catch (Exception exception) {
      // Log failure
      // TODO: 3697790
      FLog.w(TAG, exception, "Failed to schedule disk-cache clear");
      mCacheErrorLogger
          .logError(CacheErrorLogger.CacheErrorCategory.GENERIC_IO, TAG, "clearAll", exception);
      return Task.forError(exception);
    }
  }

  public void trimMemory() {
    for (Entry<Integer, WeakReference<MemoryTrimmable>> pool : mMemoryPool.entrySet()) {
      if (pool.getValue().get() != null) {
        pool.getValue().get().trim(MemoryTrimType.OnAppBackgrounded);
      }
    }
  }

  public long getSize() {
    return mFileCache.getSize();
  }

  public PooledByteBufferFactory getPooledByteBufferFactory() {
    return mPooledByteBufferFactory;
  }

  private Task<FileEntry> foundFileEntry(CacheKey key, FileEntry fileEntry) {
    FLog.v(TAG, "Found entry for %s in staging area", key.getUriString());
    return Task.forResult(fileEntry);
  }

  /**
   * Performs disk cache read. In case of any exception null is returned.
   */
  private @Nullable
  PooledByteBuffer readFromDiskCache(final CacheKey key) throws IOException {
    try {
      FLog.v(TAG, "Disk cache read for %s", key.getUriString());

      final BinaryResource diskCacheResource = mFileCache.getResource(key);
      if (!(diskCacheResource instanceof FileBinaryResource)) {
        FLog.v(TAG, "Disk cache miss for %s", key.getUriString());
        return null;
      } else {
        FLog.v(TAG, "Found entry in disk cache for %s", key.getUriString());
      }

      PooledByteBuffer byteBuffer;
      try (InputStream is = diskCacheResource.openStream()) {
        Assertions.assertCondition(is instanceof FileInputStream);
        MemoryChunk mappedMemoryChunk = new MappedMemoryChunk((FileInputStream) is,
            (int) diskCacheResource.size());
        // mmap does not need a buffer pool for alloc and dealloc.
        CloseableReference<MemoryChunk> chunkRef = CloseableReference
            .of(mappedMemoryChunk, (ResourceReleaser) MMAP_BUFFER_RELEASER);
        byteBuffer = new MemoryPooledByteBuffer(chunkRef,
            mappedMemoryChunk.getSize());
      }

      FLog.v(TAG, "Successful read from disk cache for %s", key.getUriString());
      return byteBuffer;
    } catch (IOException ioe) {
      // TODO: 3697790 log failures
      // TODO: 5258772 - uncomment line below
      // mFileCache.remove(key);
      FLog.w(TAG, ioe, "Exception reading from cache for %s", key.getUriString());
      throw ioe;
    }
  }

  /**
   * Writes to disk cache
   *
   * @throws IOException
   */
  private void writeToDiskCache(
      final CacheKey key,
      final FileEntry fileEntry) {
    FLog.v(TAG, "About to write to disk-cache for key %s", key.getUriString());
    try {
      mFileCache.insert(
          key, new WriterCallback() {
            @Override
            public void write(OutputStream os) throws IOException {
              mPooledByteStreams.copy(fileEntry.getInputStream(), os);
            }
          }
      );
      FLog.v(TAG, "Successful disk-cache write for key %s", key.getUriString());
    } catch (IOException ioe) {
      // Log failure
      // TODO: 3697790
      FLog.w(TAG, ioe, "Failed to write to disk-cache for key %s", key.getUriString());
    }
  }

  private DiskCacheConfig buildDiskCacheConfig(final String path) {
    return DiskCacheConfig.newBuilder(null)
        .setBaseDirectoryPathSupplier(() -> new File(path + STORAGE_PATH))
        .setMaxCacheSize(MAX_CACHE_SIZE)
        .setVersion(DB_VERSION)
        .setCacheEventListener(mCacheEventListener)
        .setCacheErrorLogger(mCacheErrorLogger)
        .setIndexPopulateAtStartupEnabled(true)
        .build();
  }

  private PoolConfig buildPoolConfig() {
    return PoolConfig.newBuilder()
        .setMemoryTrimmableRegistry(new MemoryTrimmableRegistry() {
          @Override
          public void registerMemoryTrimmable(MemoryTrimmable trimmable) {
            mMemoryPool.put(trimmable.hashCode(), new WeakReference<>(trimmable));
          }

          @Override
          public void unregisterMemoryTrimmable(MemoryTrimmable trimmable) {
            mMemoryPool.remove(trimmable.hashCode());
          }
        }).build();
  }
}
